{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed6a7cd0-ad5b-4cfe-ba36-9e46db8b5153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### File: performance_analysis_full_updated.ipynb\n",
    "#### Created with assistance of ChatGPT (OpenAI) – reviewed on 2025-05-11\n",
    "#### Author: Maria Heinrich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8693e933-df2f-49c4-9ce5-007365776424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Load your execution times CSV\n",
    "df = pd.read_csv(\"Execution_Times.csv\")  # Adjust the path as needed\n",
    "\n",
    "# Parse Environment and Date from Filename\n",
    "def parse_metadata(filename):\n",
    "    if \"Notebook\" in filename:\n",
    "        environment = \"Fabric\"\n",
    "    else:\n",
    "        environment = \"Databricks\"\n",
    "\n",
    "    if \"_0505\" in filename or \"_21_\" in filename:\n",
    "        date = \"May 5\"\n",
    "    elif \"_0506\" in filename or \"_22_\" in filename:\n",
    "        date = \"May 6\"\n",
    "    elif \"_250511_\" in filename or \"_18_\" in filename:\n",
    "        date = \"May 11\"\n",
    "    else:\n",
    "        date = \"Unknown\"\n",
    "\n",
    "    return pd.Series([environment, date])\n",
    "\n",
    "# Apply classification\n",
    "df[['Environment', 'Date']] = df['Filename'].apply(parse_metadata)\n",
    "df = df.rename(columns={\"Execution time in seconds\": \"Execution Time (s)\"})\n",
    "df = df[['Environment', 'Date', 'Execution Time (s)']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "457ebd91-f3f9-4811-9756-b101704899d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=df, x=\"Date\", y=\"Execution Time (s)\", hue=\"Environment\")\n",
    "plt.title(\"Execution Time Distribution: Azure Databricks vs Microsoft Fabric\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d08f799-d5b9-4e1a-89be-993290af498d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "summary = df.groupby([\"Environment\", \"Date\"])[\"Execution Time (s)\"].agg([\"mean\", \"min\", \"max\", \"std\"]).reset_index()\n",
    "summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f23e4f0e-de86-4213-8b13-f8e8e0a7ed73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Filter dynamically from the dataframe\n",
    "db_0505 = df[(df[\"Environment\"] == \"Databricks\") & (df[\"Date\"] == \"May 5\")][\"Execution Time (s)\"]\n",
    "db_0506 = df[(df[\"Environment\"] == \"Databricks\") & (df[\"Date\"] == \"May 6\")][\"Execution Time (s)\"]\n",
    "db_0511 = df[(df[\"Environment\"] == \"Databricks\") & (df[\"Date\"] == \"May 11\")][\"Execution Time (s)\"]\n",
    "\n",
    "fab_0505 = df[(df[\"Environment\"] == \"Fabric\") & (df[\"Date\"] == \"May 5\")][\"Execution Time (s)\"]\n",
    "fab_0506 = df[(df[\"Environment\"] == \"Fabric\") & (df[\"Date\"] == \"May 6\")][\"Execution Time (s)\"]\n",
    "fab_0511 = df[(df[\"Environment\"] == \"Fabric\") & (df[\"Date\"] == \"May 11\")][\"Execution Time (s)\"]\n",
    "\n",
    "# Perform t-tests\n",
    "t_db_0505_0506, p_db_0505_0506 = stats.ttest_ind(db_0505, db_0506, equal_var=False)\n",
    "t_db_0506_0511, p_db_0506_0511 = stats.ttest_ind(db_0506, db_0511, equal_var=False)\n",
    "t_fab_0505_0506, p_fab_0505_0506 = stats.ttest_ind(fab_0505, fab_0506, equal_var=False)\n",
    "t_fab_0506_0511, p_fab_0506_0511 = stats.ttest_ind(fab_0506, fab_0511, equal_var=False)\n",
    "\n",
    "# Print results\n",
    "print(f\"Databricks May 5 vs May 6: t = {t_db_0505_0506:.2f}, p = {p_db_0505_0506:.4f}\")\n",
    "print(f\"Databricks May 6 vs May 11: t = {t_db_0506_0511:.2f}, p = {p_db_0506_0511:.4f}\")\n",
    "print(f\"Fabric May 5 vs May 6: t = {t_fab_0505_0506:.2f}, p = {p_fab_0505_0506:.4f}\")\n",
    "print(f\"Fabric May 6 vs May 11: t = {t_fab_0506_0511:.2f}, p = {p_fab_0506_0511:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49bb55c4-4a80-4f80-a869-5576a819f504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform T-Tests comparing Databricks vs Fabric for each individual day\n",
    "results = {}\n",
    "\n",
    "for day in [\"May 5\", \"May 6\", \"May 11\"]:\n",
    "    db = df[(df[\"Environment\"] == \"Databricks\") & (df[\"Date\"] == day)][\"Execution Time (s)\"]\n",
    "    fab = df[(df[\"Environment\"] == \"Fabric\") & (df[\"Date\"] == day)][\"Execution Time (s)\"]\n",
    "    t_stat, p_val = stats.ttest_ind(db, fab, equal_var=False)\n",
    "    results[day] = {\"t\": t_stat, \"p\": p_val, \"db_mean\": db.mean(), \"fab_mean\": fab.mean()}\n",
    "\n",
    "# Generate interpretation\n",
    "sophisticated_interpretation = \"### T-Test: Databricks vs Fabric by Day\\n\\n\"\n",
    "for day, r in results.items():\n",
    "    significance = \"a statistically significant difference\" if r[\"p\"] < 0.05 else \"no statistically significant difference\"\n",
    "    better_platform = \"Databricks\" if r[\"db_mean\"] < r[\"fab_mean\"] else \"Fabric\"\n",
    "    sophisticated_interpretation += (\n",
    "        f\"**{day}**:\\n\"\n",
    "        f\"- t = {r['t']:.2f}, p = {r['p']:.4f}\\n\"\n",
    "        f\"- Mean Execution Time — Databricks: {r['db_mean']:.2f}s, Fabric: {r['fab_mean']:.2f}s\\n\"\n",
    "        f\"- Result: {significance} in execution time. On average, **{better_platform}** was faster.\\n\\n\"\n",
    "    )\n",
    "\n",
    "sophisticated_interpretation.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99edd957-009f-4355-996a-ecb9aedf59bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### T-Test: Databricks vs Fabric by Day\n",
    "\n",
    "**May 5**  \n",
    "- **t** = 0.56, **p** = 0.5870  \n",
    "- **Mean Execution Time** — Databricks: 45.36s, Fabric: 43.77s  \n",
    "- **Result**: No statistically significant difference in execution time.  \n",
    "  On average, **Fabric** was faster.\n",
    "\n",
    "**May 6**  \n",
    "- **t** = -4.12, **p** = 0.0023  \n",
    "- **Mean Execution Time** — Databricks: 30.34s, Fabric: 44.66s  \n",
    "- **Result**: A statistically significant difference in execution time.  \n",
    "  On average, **Databricks** was faster.\n",
    "\n",
    "**May 11**  \n",
    "- **t** = -4.89, **p** = 0.0008  \n",
    "- **Mean Execution Time** — Databricks: 29.66s, Fabric: 44.53s  \n",
    "- **Result**: A statistically significant difference in execution time.  \n",
    "  On average, **Databricks** was faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a3ab2e-9923-4c4d-9bba-47e395bcc54a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Perform T-Test across all days combined between Databricks and Fabric\n",
    "db_all = df[df[\"Environment\"] == \"Databricks\"][\"Execution Time (s)\"]\n",
    "fab_all = df[df[\"Environment\"] == \"Fabric\"][\"Execution Time (s)\"]\n",
    "\n",
    "# T-test\n",
    "t_all, p_all = stats.ttest_ind(db_all, fab_all, equal_var=False)\n",
    "\n",
    "# Means\n",
    "db_mean = db_all.mean()\n",
    "fab_mean = fab_all.mean()\n",
    "\n",
    "# Generate interpretation\n",
    "overall_interpretation = f\"\"\"\n",
    "### T-Test: Databricks vs Fabric (All Days Combined)\n",
    "\n",
    "- t = {t_all:.2f}, p = {p_all:.4f}\n",
    "- Mean Execution Time — Databricks: {db_mean:.2f}s, Fabric: {fab_mean:.2f}s\n",
    "- Result: {'A statistically significant difference' if p_all < 0.05 else 'No statistically significant difference'} in execution time.\n",
    "  On average, **{'Databricks' if db_mean < fab_mean else 'Fabric'}** was faster across all days.\n",
    "\"\"\"\n",
    "\n",
    "overall_interpretation.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93679d6c-f342-4cc4-9af6-c8ea33fc2a43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### T-Test: Databricks vs Fabric (All Days Combined)\n",
    "\n",
    "- **t** = -4.18, **p** = 0.0001  \n",
    "- **Mean Execution Time** — Databricks: 35.12s, Fabric: 44.32s  \n",
    "- **Result**: A statistically significant difference in execution time.  \n",
    "  On average, **Databricks** was faster across all days.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "performance_analysis_full_updated",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
